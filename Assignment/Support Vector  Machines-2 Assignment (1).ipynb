{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d08fe-55fc-4474-b8ec-c42ddb3e621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning \n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231325c-3e5b-4008-9a90-3bffd21ea258",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial functions and kernel functions are both used in machine learning algorithms, particularly in the context of support vector machines (SVMs) and kernel methods. Here's the relationship between the two:\n",
    "\n",
    "1. Polynomial Kernel: A polynomial kernel is a type of kernel function commonly used in SVMs. It computes the dot product of vectors in a higher-dimensional space, without actually computing the transformation to that space. The formula for the polynomial kernel is \\( K(x, y) = (x \\cdot y + c)^d \\), where \\( x \\) and \\( y \\) are input vectors, \\( c \\) is a constant, and \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "2. Polynomial Functions: Polynomial functions are mathematical functions defined as \\( f(x) = a_n x^n + a_{n-1} x^{n-1} + ... + a_1 x + a_0 \\), where \\( a_n, a_{n-1}, ..., a_1, a_0 \\) are coefficients, and \\( n \\) is the degree of the polynomial. Polynomial functions are used to represent data in a polynomial space.\n",
    "\n",
    "The relationship between polynomial functions and polynomial kernels lies in the fact that polynomial kernels allow SVMs to operate in a higher-dimensional space efficiently without explicitly transforming the input data into that space. This higher-dimensional space can capture nonlinear relationships between data points, similar to how polynomial functions can represent nonlinear relationships between variables.\n",
    "\n",
    "In essence, polynomial kernels provide a convenient way to implicitly map the input data into a higher-dimensional space where linear separation might be more feasible, much like polynomial functions represent data in a higher-dimensional space. This relationship allows SVMs to efficiently handle nonlinear classification tasks using polynomial kernels without the need for explicitly computing the transformations associated with polynomial functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48904a0-c681-4655-840f-94a8d1c80692",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda3e8e9-9712-444f-b4a3-7aec8575ba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset (or any other dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the SVM classifier with a polynomial kernel\n",
    "# we can specify the degree of the polynomial kernel using the 'degree' parameter\n",
    "# we can also tune other parameters like 'C' for regularization and 'gamma' for kernel coefficient\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0, gamma='scale')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07c886-d909-4ea9-9ecf-b63235da2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309543d7-0511-42ee-a863-fcd07478c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Support Vector Regression (SVR), epsilon (\\( \\varepsilon \\)) is a hyperparameter that controls the margin of tolerance around the predicted value within which no penalty is associated with the training points. It defines the boundary within which errors are ignored.\n",
    "\n",
    "When you increase the value of epsilon in SVR:\n",
    "\n",
    "1. Increase in Tolerance for Errors: A larger epsilon allows for a greater tolerance for errors in prediction. It means that the SVR model is permitted to have larger deviations from the actual target values.\n",
    "\n",
    "2. Impact on Support Vectors: As epsilon increases, the number of support vectors tends to decrease. This is because larger epsilon values allow more data points to fall within the margin of tolerance, which reduces the need for support vectors to define the boundary of the margin.\n",
    "\n",
    "3. Smoother Predictions: With larger epsilon values, the SVR model is more tolerant of errors, leading to smoother predictions. This can help prevent overfitting, especially when dealing with noisy data or when the target variable has inherent variability.\n",
    "\n",
    "4. Risk of Underfitting: However, excessively large values of epsilon may lead to underfitting, where the model is too simplistic and fails to capture the underlying patterns in the data effectively.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR generally reduces the number of support vectors while allowing for smoother predictions with a higher tolerance for errors. However, it's essential to strike a balance and avoid setting epsilon too large, as it may lead to underfitting and poor performance on the dataset. The optimal value of epsilon often needs to be determined through experimentation and cross-validation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd45107-7d0f-44c5-9abe-6e6ecf80922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter \n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works \n",
    "and provide examples of when you might want to increase or decrease its value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb805d9-9ed4-49a0-82c1-53eaedda6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Support Vector Regression (SVR), several parameters affect the performance and behavior of the model. Let's discuss each parameter and its impact:\n",
    "\n",
    "1. Kernel Function:\n",
    "   - The kernel function determines the type of transformation applied to the input data, allowing SVR to capture nonlinear relationships.\n",
    "   - Examples include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "   - When to choose:\n",
    "     - Use a linear kernel when the relationship between features and the target variable is approximately linear.\n",
    "     - Use polynomial, RBF, or sigmoid kernels when the relationship is nonlinear. RBF kernel is particularly flexible and often yields good results across different datasets.\n",
    "\n",
    "2. C Parameter:\n",
    "   - The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A smaller C encourages a larger margin but allows more training errors, while a larger C penalizes errors more heavily, potentially leading to overfitting.\n",
    "   - When to increase:\n",
    "     - Increase C when you suspect that the model is underfitting and needs to fit the training data more closely.\n",
    "     - Increase C when the data is well-separated and a smaller margin is acceptable.\n",
    "   - When to decrease:\n",
    "     - Decrease C when the model is overfitting, and a larger margin is desired to generalize better to unseen data.\n",
    "\n",
    "3. Epsilon Parameter:\n",
    "   - Epsilon (\\( \\varepsilon \\)) determines the margin of tolerance around the predicted value within which no penalty is associated with the training points. It defines the boundary within which errors are ignored.\n",
    "   - A larger epsilon allows for a greater tolerance for errors in prediction.\n",
    "   - When to increase:\n",
    "     - Increase epsilon when you want to allow larger deviations from the actual target values, especially when dealing with noisy data or when you expect inherent variability in the target variable.\n",
    "   - When to decrease:\n",
    "     - Decrease epsilon if you want to impose stricter tolerances on errors, which may be appropriate for datasets where precision is crucial.\n",
    "\n",
    "4. Gamma Parameter:\n",
    "   - The gamma parameter defines the influence of a single training example. It affects the \"smoothness\" of the decision boundary and the influence range of each training example.\n",
    "   - High values of gamma lead to more complex decision boundaries, potentially resulting in overfitting.\n",
    "   - When to increase:\n",
    "     - Increase gamma when the model is underfitting and needs to capture finer details of the training data, particularly in RBF kernel.\n",
    "   - When to decrease:\n",
    "     - Decrease gamma to simplify the decision boundary and prevent overfitting, especially when dealing with a large number of noisy or redundant features.\n",
    "\n",
    "In summary, the choice of kernel function, C parameter, epsilon parameter, and gamma parameter significantly affects the performance and behavior of SVR. Understanding their roles and knowing when to increase or decrease their values is crucial for building well-performing SVR models on different datasets. Experimentation and tuning through techniques like cross-validation are often necessary to determine the optimal values for these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62742de7-8595-47e3-919a-4a87cd459823",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, \n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to \n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f283616f-9728-416f-9ecf-4b423af8abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best Parameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Train the tuned classifier on the entire dataset\u001b[39;00m\n\u001b[1;32m     44\u001b[0m best_classifier \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m---> 45\u001b[0m best_classifier\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_scaled\u001b[49m, y)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Save the trained classifier to a file\u001b[39;00m\n\u001b[1;32m     48\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(best_classifier, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm_classifier.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier (accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'linear', 'poly', 'sigmoid']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_classifier = grid_search.best_estimator_\n",
    "best_classifier.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(best_classifier, 'svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8fd9b-411f-47b0-a8bd-a4e261656809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
