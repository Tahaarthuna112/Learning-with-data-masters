{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10ed2e4-f763-4c77-8dab-e53d04e5fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the \n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the \n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e939c5-c9b7-4d40-accd-5566de6fb529",
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem, which states:\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses health insurance plan}) = \\frac{P(\\text{Uses health insurance plan | Smoker}) \\times P(\\text{Smoker})}{P(\\text{Uses health insurance plan})} \\]\n",
    "\n",
    "Given:\n",
    "- \\( P(\\text{Uses health insurance plan}) = 0.70 \\) (70% of the employees use the health insurance plan)\n",
    "- \\( P(\\text{Smoker}) = 0.40 \\) (40% of the employees who use the plan are smokers)\n",
    "- \\( P(\\text{Uses health insurance plan | Smoker}) = 1 \\) (all smokers use the health insurance plan)\n",
    "\n",
    "Substitute these values into Bayes' theorem:\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses health insurance plan}) = \\frac{1 \\times 0.40}{0.70} \\]\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses health insurance plan}) = \\frac{0.40}{0.70} \\]\n",
    "\n",
    "\\[ P(\\text{Smoker | Uses health insurance plan}) \\approx 0.5714 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.5714 or 57.14%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427a6cb-cd22-4381-bc41-8a8862b80faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2734ae-f6ca-41ac-918b-906e3252635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm commonly used in machine learning for classification tasks, but they differ in the type of data they are designed to handle and the underlying assumptions they make.\n",
    "\n",
    "1. Bernoulli Naive Bayes:\n",
    "   - Bernoulli Naive Bayes is suitable for binary or boolean features, where each feature represents a binary attribute (e.g., presence or absence of a word in a document).\n",
    "   - It assumes that the features are binary-valued (0 or 1) and that the presence or absence of each feature is independent of one another given the class label.\n",
    "   - It is commonly used in text classification tasks such as spam detection, sentiment analysis, or document categorization, where the features can be represented as binary indicators.\n",
    "   - Bernoulli Naive Bayes calculates the probability of each feature occurring given each class and then uses Bayes' theorem to compute the posterior probability of each class given the observed features.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "   - Multinomial Naive Bayes is suitable for features that represent counts or frequencies, such as word counts in text data.\n",
    "   - It assumes that the features follow a multinomial distribution (i.e., each feature represents the frequency of occurrence of a particular event).\n",
    "   - It is commonly used in text classification tasks where features are represented as word counts or term frequencies.\n",
    "   - Multinomial Naive Bayes calculates the probability of each feature occurring given each class and then uses Bayes' theorem to compute the posterior probability of each class given the observed features.\n",
    "   - It works well with text data but can also be applied to other types of data where features represent counts or frequencies.\n",
    "\n",
    "In summary, the main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of features they are designed to handle (binary vs. count/frequency) and the underlying probability distributions they assume for these features (Bernoulli vs. multinomial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2182f08-00bb-4f5d-b7d1-95450f448b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9b501-36f2-4ad2-a1d7-8c140163f6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Bernoulli Naive Bayes, missing values are typically handled by ignoring them during the calculation of probabilities. Since Bernoulli Naive Bayes assumes binary features (where each feature is either present or absent), missing values can be treated as either not present (0) or not considered in the calculation altogether.\n",
    "\n",
    "When a missing value occurs for a particular feature in a sample:\n",
    "1. If the feature is represented by a binary indicator (0 or 1):\n",
    "   - The missing value can be imputed as either 0 or 1, depending on the context or the specific implementation.\n",
    "   - In many cases, it's reasonable to assume that if the value is missing, the feature is not present (0).\n",
    "2. Alternatively, the missing value can be ignored during the probability calculation for that feature. This is often the preferred approach because it avoids making assumptions about the missing values and maintains the independence assumption of the Naive Bayes classifier.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes typically handles missing values by either imputing them with a specific value (such as 0 or 1) or by ignoring them during probability calculations, depending on the chosen strategy or implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe7f0c-61fd-4d65-b9bc-0f6a8416d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110bc73-3a08-4db1-ba9b-26acee83a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. It's particularly well-suited for continuous-valued features.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes works by estimating the parameters of the Gaussian distribution (mean and variance) for each class. Then, when classifying a new instance with multiple features, it calculates the likelihood of the observed feature values under each class's Gaussian distribution. Finally, it combines these likelihoods with the prior probabilities of each class using Bayes' theorem to determine the most likely class for the given instance.\n",
    "\n",
    "Although Gaussian Naive Bayes is commonly used for binary or two-class classification problems, it can also be extended to handle multi-class classification by applying the same principles. Each class will have its own Gaussian distribution parameters for each feature, and the classifier will select the class with the highest posterior probability given the observed feature values.\n",
    "\n",
    "So, in summary, Gaussian Naive Bayes can indeed be used for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0a9723-d496-4c36-90c5-7623719622cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
