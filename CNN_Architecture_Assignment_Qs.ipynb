{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgnkf3+xz3EjpcwqTdlQI2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahaarthuna112/Learning-with-data-masters/blob/main/CNN_Architecture_Assignment_Qs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmSraR6GyhPD"
      },
      "outputs": [],
      "source": [
        "TOPIC: Understanding Pooling and Padding in CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q1.Describe the purpose and benifits of pooling in CNN.\n",
        "---"
      ],
      "metadata": {
        "id": "tmzkSVr1ythl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Pooling is a fundamental operation in Convolutional Neural Networks (CNNs) that plays a crucial role in reducing the spatial dimensions of the input volume, thus controlling the complexity of the network and extracting key features. Pooling is typically applied after convolutional layers in CNN architectures. The two most common types of pooling are max pooling and average pooling.\n",
        "\n",
        "### Purpose of Pooling:\n",
        "\n",
        "1. **Spatial Hierarchical Representation:**\n",
        "   - Pooling helps create a hierarchical representation of the input data. By downsampling the spatial dimensions, higher-level features can be captured by grouping together lower-level features.\n",
        "\n",
        "2. **Translation Invariance:**\n",
        "   - Pooling enhances the network's ability to be somewhat invariant to small translations or spatial shifts in the input. This is important for recognizing patterns regardless of their exact location in the image.\n",
        "\n",
        "3. **Reduction of Computational Complexity:**\n",
        "   - Pooling reduces the computational complexity of the network by decreasing the spatial resolution. This leads to a smaller number of parameters and computations in subsequent layers, making the network more computationally efficient.\n",
        "\n",
        "4. **Parameter Sharing:**\n",
        "   - Pooling promotes parameter sharing. By summarizing the information in a local neighborhood through operations like max or average pooling, the network becomes less sensitive to the exact spatial location of features, thus promoting generalization.\n",
        "\n",
        "5. **Feature Generalization:**\n",
        "   - Pooling helps in generalizing learned features, making the network more robust to variations and distortions in the input data. This is particularly useful for tasks like image classification where objects can appear in different positions and scales.\n",
        "\n",
        "### Benefits of Pooling:\n",
        "\n",
        "1. **Dimensionality Reduction:**\n",
        "   - Pooling reduces the spatial dimensions of the input volume, effectively downsampling the feature maps. This reduction in dimensionality makes subsequent layers more manageable and computationally efficient.\n",
        "\n",
        "2. **Increased Receptive Field:**\n",
        "   - Pooling increases the receptive field of neurons in deeper layers. Neurons in the deeper layers correspond to larger regions of the input data due to the reduction in spatial dimensions, allowing them to capture more global and abstract features.\n",
        "\n",
        "3. **Enhanced Translation Invariance:**\n",
        "   - Pooling contributes to making the network partially invariant to translations by aggregating local information. This is particularly useful when the exact location of a feature is less important for the task.\n",
        "\n",
        "4. **Noise Robustness:**\n",
        "   - Pooling can improve the network's robustness to noise and minor distortions in the input data. By summarizing local information, the network focuses on essential features while mitigating the impact of irrelevant details.\n",
        "\n",
        "5. **Computational Efficiency:**\n",
        "   - The reduction in spatial dimensions achieved through pooling results in a decrease in the number of parameters and computations in subsequent layers, contributing to improved computational efficiency.\n",
        "\n",
        "### It's important to note that while pooling offers several benefits, excessive pooling can lead to a loss of fine-grained spatial information. The choice of pooling strategy (max pooling, average pooling) and the pooling size are hyperparameters that should be carefully tuned based on the characteristics of the data and the requirements of the task at hand.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "Sva8QAMUy3k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. Explain the difference between min pooling and max pooling.\n",
        "---"
      ],
      "metadata": {
        "id": "Zi1oScvjy7yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Max pooling and min pooling are two types of pooling operations commonly used in Convolutional Neural Networks (CNNs) for downsampling and feature extraction. The key difference between them lies in how they aggregate information from local regions of the input data.\n",
        "\n",
        "### Max Pooling:\n",
        "\n",
        "- **Operation:**\n",
        "  - In max pooling, the operation performed in each local region (pooling window) is to take the maximum value. It selects the maximum value from the set of values within the pooling window.\n",
        "\n",
        "- **Purpose:**\n",
        "  - Max pooling is designed to capture the most prominent or activated features within a local region. By selecting the maximum value, the pooled output retains the strongest feature, enhancing the network's ability to focus on important patterns.\n",
        "\n",
        "- **Benefits:**\n",
        "  - Max pooling provides translation invariance and is effective in capturing the presence of specific features regardless of their exact spatial location. It also helps in reducing spatial dimensions and computational complexity.\n",
        "\n",
        "- **Example:**\n",
        "  - Given a 2x2 pooling window, max pooling would output the maximum value from the four values in the window.\n",
        "\n",
        "### Min Pooling:\n",
        "\n",
        "- **Operation:**\n",
        "  - In min pooling, the operation is to take the minimum value within the pooling window. It selects the smallest value from the set of values within the local region.\n",
        "\n",
        "- **Purpose:**\n",
        "  - Min pooling is less common than max pooling, and its purpose is to capture the least activated or least prominent features within a local region. It can be used in specific scenarios where identifying the minimum value is relevant, but it's not as widely employed as max pooling.\n",
        "\n",
        "- **Benefits:**\n",
        "  - Min pooling may be used in situations where the presence of specific features is indicated by lower values in the input data. However, it's less commonly used in practice compared to max pooling.\n",
        "\n",
        "- **Example:**\n",
        "  - Given a 2x2 pooling window, min pooling would output the minimum value from the four values in the window.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- **Max Pooling:**\n",
        "  - Selects the maximum value from the pooling window.\n",
        "  - Captures prominent features.\n",
        "  - Commonly used for downsampling and feature extraction.\n",
        "\n",
        "- **Min Pooling:**\n",
        "  - Selects the minimum value from the pooling window.\n",
        "  - Captures less activated features (less common in practice).\n",
        "  - Used in specific scenarios where identifying the minimum value is relevant.\n",
        "\n",
        "### In most CNN architectures, max pooling is the more prevalent choice due to its effectiveness in capturing dominant features and promoting translation invariance. Min pooling is used less frequently and is typically applied in specialized contexts where the minimum value carries specific information relevant to the task.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "NRH2I9FszAGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. Discuss the concept of padding in CNN and its significance .\n",
        "---"
      ],
      "metadata": {
        "id": "MvtkfNBczCYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Padding is a technique used in Convolutional Neural Networks (CNNs) to add extra pixels (or values) around the input data before applying convolutional operations. The concept of padding has significant implications for the size of the output feature maps and the information at the edges of the input.\n",
        "\n",
        "### Significance of Padding in CNN:\n",
        "\n",
        "1. **Preserving Spatial Information:**\n",
        "   - Without padding, as convolutional operations are applied, the spatial dimensions of the feature maps tend to shrink. Padding helps in preserving the spatial information at the borders of the input, ensuring that convolutional layers do not excessively reduce the size of the feature maps.\n",
        "\n",
        "2. **Avoiding Information Loss:**\n",
        "   - Padding is particularly important when dealing with deep networks. Without padding, each layer's receptive field becomes progressively smaller, potentially leading to significant information loss, especially at the edges of the input data.\n",
        "\n",
        "3. **Handling Border Effects:**\n",
        "   - Convolutional operations without padding may cause the network to focus more on the central parts of the input, neglecting the information at the borders. Padding helps in handling border effects, ensuring that the network considers information from the entire input.\n",
        "\n",
        "4. **Centering Convolutional Kernels:**\n",
        "   - Padding allows the convolutional kernels to be centered on the input pixels. This is important because it ensures that the convolutional operation is applied symmetrically, and the central pixel of the kernel aligns with the central pixel of the receptive field.\n",
        "\n",
        "5. **Controlling Output Size:**\n",
        "   - Padding gives control over the size of the output feature maps. It allows the adjustment of the spatial dimensions based on the desired output size, which can be important for designing neural networks with specific architectural requirements.\n",
        "\n",
        "6. **Avoiding Vanishing Gradient:**\n",
        "   - Padding can help in mitigating the vanishing gradient problem, especially when using deep networks. It ensures that information from the entire input contributes to the computation of gradients during backpropagation.\n",
        "\n",
        "### Types of Padding:\n",
        "\n",
        "1. **Valid (No Padding):**\n",
        "   - No padding is applied, resulting in a reduction of spatial dimensions after convolution.\n",
        "\n",
        "2. **Same Padding:**\n",
        "   - Padding is added in such a way that the output feature map has the same spatial dimensions as the input. This is achieved by adding zeros around the input.\n",
        "\n",
        "3. **Full Padding:**\n",
        "   - Padding is added to the input such that the convolutional kernel fits entirely inside the input, and the output size is maximized.\n",
        "\n",
        "### Mathematical Representation of Padding:\n",
        "\n",
        "If $P$ is the padding size, $W_{\\text{in}}$ is the input width, and $F$ is the filter (kernel) size, the output width $W_{\\text{out}}$ with padding can be calculated using the formula:\n",
        "\n",
        "$ W_{\\text{out}} = \\frac{W_{\\text{in}} - F + 2P}{S} + 1 $\n",
        "\n",
        "where $S$ is the stride.\n",
        "\n",
        "### In summary, padding in CNNs is a crucial technique that helps maintain spatial information, reduces information loss at the edges of the input, and allows for better control over the network's behavior, especially in the context of convolutional operations. The choice of padding type depends on the specific requirements of the network architecture and the desired characteristics of the feature maps.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "vC7P1tt2zFiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size.\n",
        "---"
      ],
      "metadata": {
        "id": "MTTA5wyvzHf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Zero-padding and valid-padding are two types of padding used in Convolutional Neural Networks (CNNs), and they have distinct effects on the size of the output feature map.\n",
        "\n",
        "### Zero-padding:\n",
        "\n",
        "1. **Definition:**\n",
        "   - Zero-padding involves adding zero-valued pixels around the input data before applying convolutional operations.\n",
        "\n",
        "2. **Effect on Output Size:**\n",
        "   - Increases the spatial dimensions of the input by adding zeros around it.\n",
        "   - Preserves the spatial information at the edges of the input.\n",
        "   - The output feature map size is larger compared to the valid-padding case.\n",
        "\n",
        "3. **Mathematical Representation:**\n",
        "   - If $P$ is the zero-padding size, $W_{\\text{in}}$ is the input width, and $F$ is the filter size, the output width $W_{\\text{out}}$ with zero-padding can be calculated using the formula:\n",
        "     $ W_{\\text{out}} = \\frac{W_{\\text{in}} - F + 2P}{S} + 1 $\n",
        "\n",
        "4. **Use Cases:**\n",
        "   - Commonly used when preserving spatial information at the borders of the input is important.\n",
        "   - Used in the \"same\" padding configuration to make the output size match the input size.\n",
        "\n",
        "### Valid-padding:\n",
        "\n",
        "1. **Definition:**\n",
        "   - Valid-padding (sometimes referred to as \"no padding\") involves not adding any extra pixels around the input data before applying convolutional operations.\n",
        "\n",
        "2. **Effect on Output Size:**\n",
        "   - Does not add any extra pixels, resulting in a reduction of spatial dimensions after convolution.\n",
        "   - The output feature map size is smaller compared to the zero-padding case.\n",
        "\n",
        "3. **Mathematical Representation:**\n",
        "   - If no padding is applied ($P = 0$), the output width $W_{\\text{out}}$ can be calculated using the formula:\n",
        "     $ W_{\\text{out}} = \\frac{W_{\\text{in}} - F}{S} + 1 $\n",
        "\n",
        "4. **Use Cases:**\n",
        "   - Commonly used when reducing spatial dimensions and extracting key features are the primary goals.\n",
        "   - Leads to a more compact representation of the input.\n",
        "\n",
        "### Comparison:\n",
        "\n",
        "- **Effect on Output Size:**\n",
        "  - Zero-padding increases the output size, while valid-padding reduces the output size.\n",
        "\n",
        "- **Spatial Information:**\n",
        "  - Zero-padding preserves spatial information at the edges, making it suitable when maintaining information near the borders is important.\n",
        "  - Valid-padding may lead to information loss at the edges due to the reduction in spatial dimensions.\n",
        "\n",
        "- **Use Cases:**\n",
        "  - Zero-padding is often used in scenarios where maintaining spatial information is crucial, such as in tasks where edge details matter.\n",
        "  - Valid-padding is commonly employed when the goal is to reduce spatial dimensions and capture essential features, especially in deep architectures.\n",
        "\n",
        "- **Computational Efficiency:**\n",
        "  - Zero-padding increases computational complexity due to the larger input size.\n",
        "  - Valid-padding reduces computational complexity, making it computationally more efficient.\n",
        "\n",
        "### In summary, the choice between zero-padding and valid-padding depends on the specific requirements of the neural network architecture and the desired characteristics of the feature maps. Zero-padding is beneficial when spatial information preservation is a priority, while valid-padding is suitable for tasks where compact feature representations and reduced spatial dimensions are preferred.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "7ot8VCcEzKx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC: Exploring LeNet"
      ],
      "metadata": {
        "id": "_tYseq_9zVtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q1. Provide a brief overview of LeNet-5 architecture\n",
        "---"
      ],
      "metadata": {
        "id": "d45s6cb4zYh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LeNet-5 is a pioneering convolutional neural network (CNN) architecture designed by Yann LeCun and his collaborators in 1998. It was one of the earliest successful attempts at using deep learning for image recognition and classification. LeNet-5 was specifically developed for handwritten digit recognition and played a significant role in the advancement of deep learning.\n",
        "\n",
        "### Overview of LeNet-5 Architecture:\n",
        "\n",
        "1. **Input Layer:**\n",
        "   - LeNet-5 takes as input grayscale images of size 32x32 pixels. In the original application, it was primarily used for recognizing handwritten digits.\n",
        "\n",
        "2. **Convolutional Layers:**\n",
        "   - LeNet-5 consists of two convolutional layers:\n",
        "      - **Convolutional Layer 1:**\n",
        "         - Convolution with a 5x5 kernel.\n",
        "         - Sigmoid activation function.\n",
        "         - Subsampling (average pooling) with a 2x2 window and a stride of 2.\n",
        "      - **Convolutional Layer 2:**\n",
        "         - Convolution with a 5x5 kernel.\n",
        "         - Sigmoid activation function.\n",
        "         - Subsampling (average pooling) with a 2x2 window and a stride of 2.\n",
        "\n",
        "3. **Fully Connected Layers:**\n",
        "   - LeNet-5 has three fully connected layers:\n",
        "      - **Fully Connected Layer 1:**\n",
        "         - 120 neurons.\n",
        "         - Sigmoid activation function.\n",
        "      - **Fully Connected Layer 2:**\n",
        "         - 84 neurons.\n",
        "         - Sigmoid activation function.\n",
        "      - **Output Layer:**\n",
        "         - Number of neurons depends on the classification task.\n",
        "         - Typically, for handwritten digit recognition, there are 10 neurons (one for each digit 0-9).\n",
        "         - Sigmoid or softmax activation function.\n",
        "\n",
        "4. **Activation Function:**\n",
        "   - Sigmoid activation functions were used in the original LeNet-5 architecture.\n",
        "\n",
        "5. **Training:**\n",
        "   - LeNet-5 was trained using the gradient-based optimization algorithm, backpropagation, and stochastic gradient descent.\n",
        "\n",
        "6. **Architecture Summary:**\n",
        "   - Input: 32x32 grayscale images.\n",
        "   - Convolutional layers with 5x5 kernels.\n",
        "   - Subsampling (average pooling) layers.\n",
        "   - Fully connected layers with sigmoid activation.\n",
        "   - Output layer with 10 neurons for digit classification.\n",
        "\n",
        "### Contributions and Impact:\n",
        "\n",
        "1. **Handwritten Digit Recognition:**\n",
        "   - LeNet-5 demonstrated significant success in recognizing handwritten digits, particularly in the context of the MNIST dataset.\n",
        "\n",
        "2. **Convolutional Neural Network Pioneer:**\n",
        "   - LeNet-5 is considered one of the pioneering architectures in the development of convolutional neural networks, laying the foundation for subsequent advancements in deep learning.\n",
        "\n",
        "3. **Inspiration for Modern Architectures:**\n",
        "   - The principles and concepts introduced in LeNet-5 influenced the design of modern CNN architectures, including the use of convolutional layers, pooling layers, and fully connected layers.\n",
        "\n",
        "### While LeNet-5's architecture may appear simple compared to more recent deep learning models, it was a groundbreaking work that demonstrated the power of deep neural networks for image classification tasks, paving the way for the development of more complex and powerful architectures.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "fYvEiMXlzarB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. Describe the key components of LeNet-5 and their respective purpose .\n",
        "---"
      ],
      "metadata": {
        "id": "gy6bAgJGzv48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LeNet-5 consists of several key components, each serving a specific purpose in the architecture. Here's a breakdown of the main components and their roles:\n",
        "\n",
        "1. **Input Layer:**\n",
        "   - **Purpose:** The input layer is where the raw input data is fed into the network. In the case of LeNet-5, the input consists of grayscale images with dimensions of 32x32 pixels. Each pixel value represents the intensity of the corresponding pixel in the image.\n",
        "\n",
        "2. **Convolutional Layers:**\n",
        "   - **Purpose:** The convolutional layers are responsible for extracting local features from the input images. LeNet-5 has two convolutional layers, each followed by subsampling (average pooling) layers.\n",
        "     - **Convolutional Layer 1:**\n",
        "       - **Convolution:** Applies convolution with a 5x5 kernel to detect simple patterns.\n",
        "       - **Activation:** Applies a sigmoid activation function to introduce non-linearity.\n",
        "       - **Subsampling (Average Pooling):** Reduces spatial dimensions (downsampling) by taking the average of non-overlapping 2x2 regions.\n",
        "     - **Convolutional Layer 2:**\n",
        "       - Similar to the first convolutional layer but with different weights.\n",
        "       - **Convolution, Activation, Subsampling (Average Pooling):** Similar operations as in the first convolutional layer.\n",
        "\n",
        "3. **Fully Connected Layers:**\n",
        "   - **Purpose:** The fully connected layers are responsible for capturing high-level features and making decisions based on the learned representations.\n",
        "     - **Fully Connected Layer 1:**\n",
        "       - **Neurons:** 120 neurons.\n",
        "       - **Activation:** Sigmoid activation function.\n",
        "       - **Purpose:** Captures complex combinations of features extracted by the convolutional layers.\n",
        "     - **Fully Connected Layer 2:**\n",
        "       - **Neurons:** 84 neurons.\n",
        "       - **Activation:** Sigmoid activation function.\n",
        "       - **Purpose:** Further refines the learned features for classification.\n",
        "     - **Output Layer:**\n",
        "       - **Neurons:** The number of neurons in the output layer depends on the classification task. For handwritten digit recognition, it is typically 10 neurons (one for each digit).\n",
        "       - **Activation:** Sigmoid or softmax activation function.\n",
        "       - **Purpose:** Produces the final output of the network, representing the predicted class probabilities.\n",
        "\n",
        "4. **Activation Function (Sigmoid):**\n",
        "   - **Purpose:** Sigmoid activation functions introduce non-linearity to the network, allowing it to learn complex relationships in the data. While modern architectures often use other activation functions like ReLU, LeNet-5 employed sigmoid activation functions.\n",
        "\n",
        "5. **Subsampling (Average Pooling):**\n",
        "   - **Purpose:** Subsampling, achieved through average pooling, reduces the spatial dimensions of the feature maps. This downsampling helps in preserving the most important information while reducing computational complexity and avoiding overfitting.\n",
        "\n",
        "6. **Training Procedure:**\n",
        "   - **Purpose:** LeNet-5 is trained using backpropagation and stochastic gradient descent to minimize a predefined loss function. The training process adjusts the weights of the network to minimize the difference between predicted and actual class labels.\n",
        "\n",
        "7. **Loss Function:**\n",
        "   - **Purpose:** The loss function measures the difference between the predicted and actual class labels during training. Common loss functions include cross-entropy loss for classification tasks.\n",
        "\n",
        "8. **Output Layer Activation (Sigmoid or Softmax):**\n",
        "   - **Purpose:** The activation function in the output layer transforms the raw output values into probabilities or scores. Sigmoid activation is used for binary classification tasks, while softmax activation is employed for multi-class classification tasks.\n",
        "\n",
        "### In summary, LeNet-5's key components include convolutional layers for feature extraction, fully connected layers for high-level feature capture, activation functions for introducing non-linearity, subsampling for downsampling feature maps, and an output layer for making final predictions. The architecture was specifically designed for handwritten digit recognition and played a foundational role in the development of convolutional neural networks.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "L6LFcAVMz9nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.\n",
        "---"
      ],
      "metadata": {
        "id": "tj1Ty2iq0Ad2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LeNet-5, introduced by Yann LeCun and his colleagues in 1998, is a convolutional neural network (CNN) architecture designed for handwritten digit recognition. While it may seem outdated compared to more modern architectures like ResNet or Inception, LeNet-5 played a crucial role in the development of deep learning for image classification. Let's discuss its advantages and limitations in the context of image classification tasks.\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "1. **Pioneering Architecture:**\n",
        "   - LeNet-5 was one of the earliest successful CNN architectures, laying the foundation for subsequent developments in deep learning for image classification.\n",
        "\n",
        "2. **Spatial Hierarchies:**\n",
        "   - It leverages a hierarchical structure, utilizing multiple convolutional and subsampling layers to capture spatial hierarchies in the input data. This helps in learning hierarchical features at different levels of abstraction.\n",
        "\n",
        "3. **Weight Sharing:**\n",
        "   - LeNet-5 uses weight sharing through convolutional layers, which reduces the number of parameters in the network. This aids in better generalization, especially when dealing with limited training data.\n",
        "\n",
        "4. **Pooling Layers:**\n",
        "   - The inclusion of pooling layers (subsampling or max-pooling) helps in reducing the spatial resolution of the input, making the network more robust to translations and distortions in the input images.\n",
        "\n",
        "5. **Efficiency on Small Datasets:**\n",
        "   - LeNet-5 performs well on relatively small datasets, making it suitable for applications where large labeled datasets are not readily available.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "1. **Complexity for Modern Data:**\n",
        "   - LeNet-5 may struggle with more complex datasets with diverse patterns and structures. Modern datasets, such as those used in competitions like ImageNet, contain more intricate objects and scenes that demand deeper and more complex architectures.\n",
        "\n",
        "2. **Limited Capacity:**\n",
        "   - The model has limited capacity compared to more recent architectures. It may not capture the high-level features required for state-of-the-art performance on challenging tasks.\n",
        "\n",
        "3. **Activation Functions:**\n",
        "   - LeNet-5 primarily uses sigmoid activation functions, which have some limitations, such as susceptibility to the vanishing gradient problem. Modern architectures often use rectified linear units (ReLUs) for more effective learning.\n",
        "\n",
        "4. **Not Suited for Large-Scale Image Datasets:**\n",
        "   - LeNet-5 was designed for smaller image datasets, and its architecture may not scale well to handle the increased complexity and size of contemporary datasets like ImageNet.\n",
        "\n",
        "5. **Limited Receptive Field:**\n",
        "   - The receptive field of the neurons in LeNet-5 is relatively small, which may limit its ability to capture long-range dependencies and global context in large images.\n",
        "\n",
        "### In summary, while LeNet-5 was groundbreaking at the time of its introduction, its limitations make it less suitable for modern image classification tasks on large, complex datasets. Advances in deep learning have led to more sophisticated architectures that address these limitations and achieve superior performance on a wide range of tasks.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "hc4y-ELw0Hq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. Implement LeNet-5 using a deep learning framework of your choice (eg. Tensorflow, Pytorch) and train it on publicly available dataset (eg. MNIST) Evaluate its performance and provide insights\n",
        "---"
      ],
      "metadata": {
        "id": "H1mu8Rwf0NMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "0TPxAYPk0OAB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ien4imd0QrJ",
        "outputId": "5912f369-aec6-41c6-ba98-5f0d13b0ec77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "035n0on_0bX_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LeNet-5 model architecture\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(16, (5, 5), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(120, activation='relu'))\n",
        "model.add(layers.Dense(84, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJL_vSQD0dEk",
        "outputId": "6fa8d46a-4903-48b1-9ec7-2322ebada018"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "S8pU0CM10gWV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N568T-oA0iq9",
        "outputId": "6b061469-48b9-49dd-c102-874adf720920"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 53ms/step - accuracy: 0.7612 - loss: 0.8042 - val_accuracy: 0.9653 - val_loss: 0.1182\n",
            "Epoch 2/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 48ms/step - accuracy: 0.9631 - loss: 0.1173 - val_accuracy: 0.9745 - val_loss: 0.0855\n",
            "Epoch 3/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 48ms/step - accuracy: 0.9756 - loss: 0.0745 - val_accuracy: 0.9768 - val_loss: 0.0787\n",
            "Epoch 4/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.9814 - loss: 0.0604 - val_accuracy: 0.9821 - val_loss: 0.0595\n",
            "Epoch 5/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 48ms/step - accuracy: 0.9845 - loss: 0.0484 - val_accuracy: 0.9842 - val_loss: 0.0527\n",
            "Epoch 6/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 52ms/step - accuracy: 0.9876 - loss: 0.0411 - val_accuracy: 0.9859 - val_loss: 0.0489\n",
            "Epoch 7/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 48ms/step - accuracy: 0.9882 - loss: 0.0363 - val_accuracy: 0.9861 - val_loss: 0.0472\n",
            "Epoch 8/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.9906 - loss: 0.0287 - val_accuracy: 0.9878 - val_loss: 0.0418\n",
            "Epoch 9/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 51ms/step - accuracy: 0.9923 - loss: 0.0238 - val_accuracy: 0.9868 - val_loss: 0.0449\n",
            "Epoch 10/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 52ms/step - accuracy: 0.9937 - loss: 0.0212 - val_accuracy: 0.9867 - val_loss: 0.0463\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cc683d7f6a0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loss and accuracy\n",
        "model.evaluate(train_images, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOCUsMhq0lP1",
        "outputId": "3f8928a6-5ced-4ff3-fb0e-6bc84763a2f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9929 - loss: 0.0212\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0259208045899868, 0.9917166829109192]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loss and accuracy\n",
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1XTBnu81V-A",
        "outputId": "6ddfd835-d621-4ab3-8764-9b694e414a4f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9859 - loss: 0.0442\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.037169717252254486, 0.9876999855041504]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### This code defines and trains the LeNet-5 model on the MNIST dataset, which consists of 28x28 grayscale images of handwritten digits (0-9). The model is compiled with the Adam optimizer and categorical crossentropy loss. After training, it evaluates the model on the train and test set and prints the train and test accuracy.\n",
        "\n",
        "### Train accuracy is around 99.28% while Testing accuracy is 98.69% which are very good results\n",
        "\n",
        "### Keep in mind that MNIST is a relatively simple dataset, and LeNet-5 may not showcase its full potential on more complex datasets\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "BI3T_dEa1Zzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOPIC: Analyzing AlexNet"
      ],
      "metadata": {
        "id": "rnCorna51fXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q1. Present an overview of Alexnet architecture\n",
        "---"
      ],
      "metadata": {
        "id": "K8F76YG81kWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### AlexNet is a deep convolutional neural network architecture that gained significant attention and marked a breakthrough in the field of computer vision, particularly in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. AlexNet demonstrated the effectiveness of deep learning in image classification tasks and played a crucial role in popularizing deep neural networks.\n",
        "\n",
        "### Here's an overview of the AlexNet architecture:\n",
        "\n",
        "1. **Input Layer:**\n",
        "   - AlexNet takes as input RGB images of size 224x224. It was designed to process larger input images compared to previous models, allowing for more detailed feature extraction.\n",
        "\n",
        "2. **Convolutional Layers:**\n",
        "   - The network starts with five convolutional layers, which are responsible for learning hierarchical features from the input images.\n",
        "   - The convolutional layers use small receptive fields (filter sizes), such as 3x3 and 5x5, with a stride of 1.\n",
        "   - The first convolutional layer has 96 filters, and subsequent layers have 256, 384, 384, and 256 filters, respectively.\n",
        "   - Rectified Linear Units (ReLU) activation functions are applied after each convolutional layer, introducing non-linearity.\n",
        "\n",
        "3. **Max Pooling Layers:**\n",
        "   - Between the convolutional layers, there are three max-pooling layers that help reduce spatial dimensions and computational load.\n",
        "   - The pooling layers use a 3x3 window with a stride of 2.\n",
        "\n",
        "4. **Local Response Normalization (LRN):**\n",
        "   - LRN is applied after the first and second convolutional layers to normalize the responses and enhance the model's generalization.\n",
        "\n",
        "5. **Flatten Layer:**\n",
        "   - After the convolutional and pooling layers, the output is flattened into a vector to be fed into fully connected layers.\n",
        "\n",
        "6. **Fully Connected Layers:**\n",
        "   - AlexNet has three fully connected layers with 4096 neurons each.\n",
        "   - The first two fully connected layers have ReLU activation functions.\n",
        "   - The final fully connected layer produces the output predictions, and it uses the softmax activation function for multi-class classification.\n",
        "\n",
        "7. **Dropout:**\n",
        "   - Dropout is applied to the first two fully connected layers with a dropout rate of 0.5 during training to prevent overfitting.\n",
        "\n",
        "8. **Output Layer:**\n",
        "   - The output layer consists of 1000 neurons (corresponding to the 1000 ImageNet classes), and it uses softmax activation for classification.\n",
        "\n",
        "9. **Training Details:**\n",
        "   - AlexNet was trained using stochastic gradient descent (SGD) with momentum.\n",
        "   - Data augmentation, such as random cropping and flipping, was employed to improve generalization.\n",
        "\n",
        "### AlexNet achieved a top-5 error rate of about 16.4% in the ImageNet competition, outperforming traditional computer vision methods and demonstrating the power of deep convolutional neural networks for image classification tasks. The success of AlexNet paved the way for the development of deeper and more sophisticated architectures in the subsequent years.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "jo9tuxmN1pI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q2. Explain architecutral innovations introduced in AlexNet that contributed its breakthrough performance.\n",
        "---"
      ],
      "metadata": {
        "id": "hTNJCXeN1wua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### AlexNet introduced several architectural innovations that contributed to its breakthrough performance in image classification tasks, particularly in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. These innovations were pivotal in demonstrating the effectiveness of deep convolutional neural networks (CNNs) for computer vision. Here are the key architectural innovations in AlexNet:\n",
        "\n",
        "1. **Deep Architecture:**\n",
        "   - AlexNet was one of the first deep convolutional neural networks, featuring eight layers. This depth allowed the network to learn hierarchical features and representations of increasing complexity.\n",
        "\n",
        "2. **Rectified Linear Units (ReLU) Activation:**\n",
        "   - AlexNet replaced traditional activation functions like sigmoid or hyperbolic tangent with ReLU activation functions after each convolutional layer. ReLU introduces non-linearity to the model and helps alleviate the vanishing gradient problem, enabling faster training and better convergence.\n",
        "\n",
        "3. **Local Response Normalization (LRN):**\n",
        "   - Local Response Normalization was applied after the first and second convolutional layers. LRN helps normalize the responses of neighboring neurons, enhancing the model's generalization and response to variations in input.\n",
        "\n",
        "4. **Large Convolutional Filters:**\n",
        "   - AlexNet used relatively large convolutional filters, including 11x11 and 5x5 filters. The larger receptive fields allowed the network to capture complex patterns and learn richer spatial hierarchies.\n",
        "\n",
        "5. **Overlapping Max Pooling:**\n",
        "   - The max-pooling layers in AlexNet used a 3x3 window with a stride of 2, resulting in overlapping pooling regions. Overlapping pooling helps in better translation invariance and contributes to the robustness of the model.\n",
        "\n",
        "6. **Multiple GPUs for Parallel Processing:**\n",
        "   - To handle the computational demands of training a deep network, AlexNet was designed to be run on two GPUs in parallel. This approach allowed the model to be trained more efficiently and reduced the training time.\n",
        "\n",
        "7. **Data Augmentation:**\n",
        "   - AlexNet employed data augmentation techniques during training, such as random cropping and flipping of input images. Data augmentation helped the model generalize better to variations in the input data and reduce overfitting.\n",
        "\n",
        "8. **Dropout:**\n",
        "   - Dropout was applied to the first two fully connected layers during training. Dropout randomly drops out a fraction of neurons during each training iteration, preventing overfitting and improving the model's generalization to unseen data.\n",
        "\n",
        "9. **Large-Scale Multiclass Classification:**\n",
        "   - AlexNet was designed for large-scale multiclass classification tasks. It was trained on the ImageNet dataset, which consists of over a million images distributed across 1000 classes. The ability to handle such a large and diverse dataset contributed to its success.\n",
        "\n",
        "### The combination of these architectural innovations made AlexNet a groundbreaking model, significantly outperforming previous approaches in the ILSVRC 2012 competition and demonstrating the potential of deep learning for image classification on a large scale. The success of AlexNet paved the way for the development of deeper and more sophisticated neural network architectures in subsequent years.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "9LmPGLzx1ykX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q3. Discuss the role of convolutional layers, pooling layers and fully connected layers in AlexNet.\n",
        "---"
      ],
      "metadata": {
        "id": "O1cd3LyP10qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### AlexNet's architecture is composed of convolutional layers, pooling layers, and fully connected layers, each playing a distinct role in feature extraction, spatial reduction, and high-level representation. Here's an overview of the roles of these components in AlexNet:\n",
        "\n",
        "### 1. Convolutional Layers:\n",
        "   - **Feature Extraction:** The convolutional layers in AlexNet are responsible for learning hierarchical features from the input images. They employ filters (kernels) to convolve across the input image, capturing local patterns and features. The use of multiple convolutional layers allows the network to learn increasingly complex and abstract representations.\n",
        "\n",
        "   - **Non-Linearity with ReLU:** After each convolutional operation, the Rectified Linear Unit (ReLU) activation function is applied. ReLU introduces non-linearity, allowing the model to learn and represent complex relationships within the data. ReLU activation helps mitigate the vanishing gradient problem and accelerates training.\n",
        "\n",
        "   - **Large Receptive Fields:** AlexNet uses relatively large convolutional filters, such as 11x11 and 5x5, in the early layers. This choice enables the network to capture larger and more complex spatial structures in the input images.\n",
        "\n",
        "### 2. Pooling Layers:\n",
        "   - **Spatial Reduction:** Pooling layers, specifically max-pooling in AlexNet, are employed to reduce the spatial dimensions of the feature maps. Max-pooling extracts the most important information from local regions, discarding less relevant details. This helps in achieving translational invariance and reducing the computational load in subsequent layers.\n",
        "\n",
        "   - **Overlapping Pooling:** AlexNet uses overlapping max-pooling with a 3x3 window and a stride of 2. Overlapping pooling regions contribute to better translation invariance and improve the model's robustness to variations in the input.\n",
        "\n",
        "### 3. Fully Connected Layers:\n",
        "   - **High-Level Representation:** The fully connected layers in AlexNet process the high-level features extracted by the convolutional and pooling layers. These layers are responsible for capturing global patterns and relationships across the entire image.\n",
        "\n",
        "   - **Non-Linearity with ReLU:** Similar to the convolutional layers, the fully connected layers use ReLU activation functions to introduce non-linearity into the network.\n",
        "\n",
        "   - **Classification Output:** The final fully connected layer produces the output for classification. In the case of AlexNet, it has 1000 neurons corresponding to the 1000 classes in the ImageNet dataset. The softmax activation function is applied to convert the output into probability scores, determining the predicted class.\n",
        "\n",
        "### Additional Considerations:\n",
        "   - **Local Response Normalization (LRN):** LRN is applied after the first and second convolutional layers in AlexNet. It normalizes the responses of neighboring neurons, enhancing the model's generalization.\n",
        "\n",
        "   - **Dropout:** Dropout is applied to the first two fully connected layers during training. Dropout helps prevent overfitting by randomly dropping out a fraction of neurons during each training iteration.\n",
        "\n",
        "### In summary, the convolutional layers focus on local feature extraction, the pooling layers reduce spatial dimensions and increase translational invariance, and the fully connected layers capture global patterns and produce the final classification output. The combination of these layers in AlexNet forms a powerful deep neural network architecture for image classification tasks.\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "o-kNsfGG14RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Q4. Implement Alexnet using a deeplearning framework of your choice and evaluate its performance on dataset of your choice.\n",
        "---"
      ],
      "metadata": {
        "id": "TbQ6HT3K16IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "Fg8NMHvk181g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeAAyesr1_xs",
        "outputId": "af41fb3f-24a0-4110-9297-f6580fb844cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AlexNet model architecture\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu',padding='same' ,input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "model.add(layers.Conv2D(256, (5, 5),strides=(1, 1), padding='same', activation='relu'))\n",
        "model.add(layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "model.add(layers.Conv2D(384, (3, 3), strides=(1, 1),padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(384, (3, 3), strides=(1, 1),padding='same', activation='relu'))\n",
        "model.add(layers.Conv2D(256, (3, 3), strides=(1, 1),padding='same', activation='relu'))\n",
        "model.add(layers.MaxPooling2D((3, 3), strides=(2,2), padding='same'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1000, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "T_T15In22B3p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uWAUAKsP2G-0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "hist = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "rB566g-D2JoZ",
        "outputId": "69216a2f-280d-4899-d0d4-53d7c2a8a946"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 2s/step - accuracy: 0.1510 - loss: 2.1701 - val_accuracy: 0.2335 - val_loss: 1.9162\n",
            "Epoch 2/10\n",
            "\u001b[1m  3/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:13\u001b[0m 2s/step - accuracy: 0.2509 - loss: 1.9068"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1f565dbd7e66>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##the above model was taking a lot of time to improve its accruacy  10+ plus epochs were needed"
      ],
      "metadata": {
        "id": "le3PL6Te2ND4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(hist.history['loss'],label='Train Loss')\n",
        "plt.plot(hist.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Categorical Cross Entropy')\n",
        "plt.title('AlexNet Learning Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "50cDWeX14mB1",
        "outputId": "5d3a8efa-ae73-4310-f7b8-c9c7a2248d18"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hist' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-680dabfd7709>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To1if9hX4tXV",
        "outputId": "fd4e5742-63aa-48cc-e2fa-405b9f09cb33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 44ms/step - accuracy: 0.2262 - loss: 1.9216\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9221818447113037, 0.2215999960899353]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oN35lz9p4zzo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}