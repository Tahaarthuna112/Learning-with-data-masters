{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmAhQcj1SGsjyMiqNnJuq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tahaarthuna112/Learning-with-data-masters/blob/main/Weight_Initialization_Assignment_Qs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkOuA1Nnx4HP"
      },
      "outputs": [],
      "source": [
        "Part 1: Upder`tapdipg Weight Ipitializatioo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1) Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize\n",
        "the weights carefully"
      ],
      "metadata": {
        "id": "isAjDeJLyBmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Weight initialization is a crucial step in training artificial neural networks (ANNs) because it significantly affects the model's ability to learn effectively and efficiently. Here’s why it’s necessary to initialize weights carefully:\n",
        "\n",
        "### 1. **Avoiding Vanishing or Exploding Gradients**\n",
        "   - **Vanishing gradients**: If weights are too small, the gradients (used in backpropagation to update the weights) can shrink to near zero as they propagate through the network. This can cause layers, especially the deeper ones, to stop learning because the weight updates become negligible.\n",
        "   - **Exploding gradients**: Conversely, if weights are initialized too large, the gradients can grow excessively as they propagate backward. This causes the model's parameters to oscillate wildly and prevents convergence.\n",
        "\n",
        "   Careful initialization helps maintain stable gradients throughout training, which is critical for deep networks.\n",
        "\n",
        "### 2. **Breaking Symmetry**\n",
        "   - If all weights are initialized to the same values, especially zeros, all neurons in the same layer will have the same output during forward propagation and will receive identical updates during backpropagation. This **symmetry** prevents the network from learning diverse features, limiting the model's ability to generalize.\n",
        "   - Random initialization breaks this symmetry, allowing neurons to learn different patterns.\n",
        "\n",
        "### 3. **Faster Convergence**\n",
        "   - Proper weight initialization can lead to **faster convergence** during training by providing a good starting point for the optimization process. It allows the network to reach an optimal solution more efficiently and reduces training time.\n",
        "   - Methods like **Xavier initialization** (for tanh or sigmoid activations) and **He initialization** (for ReLU or variants) are commonly used because they ensure the variance of activations remains stable across layers, facilitating efficient learning.\n",
        "\n",
        "### 4. **Preventing Bias**\n",
        "   - Inappropriate initialization, especially with extremely small or large weights, can introduce biases in the model, where some neurons dominate the learning process while others contribute little. Careful initialization ensures that all neurons are trained more equitably, promoting balanced learning.\n",
        "\n",
        "### Conclusion:\n",
        "Careful weight initialization is necessary to ensure stable and efficient learning, avoid common pitfalls like vanishing or exploding gradients, and help neural networks converge faster and more effectively. It’s a fundamental step that can make or break the performance of deep learning models."
      ],
      "metadata": {
        "id": "xJ670IREyIO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2) Describe the challenges associated with improper weight initialization. How do these issues affect model\n",
        "training and convergence?"
      ],
      "metadata": {
        "id": "bwuBEeXk_UyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Improper weight initialization in artificial neural networks (ANNs) introduces several challenges that negatively affect model training and convergence. Below are the key issues associated with improper initialization and how they affect the model's performance:\n",
        "\n",
        "### 1. **Vanishing Gradients**\n",
        "   - **Problem**: When weights are initialized to very small values, the gradients (which are the basis for updating the weights) can shrink during backpropagation. This is especially problematic in deep networks because the gradients become smaller with each layer.\n",
        "   - **Effect on Training**: As a result, weight updates become negligible in deeper layers, causing the model to stop learning efficiently or entirely. This slows down convergence, and the model might struggle to escape suboptimal solutions.\n",
        "   - **Consequence**: Training becomes extremely slow, or the model fails to improve beyond a certain point.\n",
        "\n",
        "### 2. **Exploding Gradients**\n",
        "   - **Problem**: If the weights are initialized too large, the gradients can grow exponentially as they propagate backward through the network.\n",
        "   - **Effect on Training**: This leads to large, erratic weight updates, causing the model's parameters to oscillate or diverge, which destabilizes the learning process.\n",
        "   - **Consequence**: The model may never converge or may require a significantly smaller learning rate to stabilize, which slows down training.\n",
        "\n",
        "### 3. **Slow Convergence**\n",
        "   - **Problem**: Poorly chosen initial weights, even if they don’t cause vanishing or exploding gradients, can still lead to slower convergence. For instance, weights that are too far from optimal require more iterations for the optimizer to adjust them to reasonable values.\n",
        "   - **Effect on Training**: The model takes longer to reach an optimal solution, consuming more computational resources and time.\n",
        "   - **Consequence**: Prolonged training time can make the process inefficient, especially with large datasets and complex networks.\n",
        "\n",
        "### 4. **Difficulty in Escaping Saddle Points or Local Minima**\n",
        "   - **Problem**: Improper weight initialization can trap the model in regions of the loss function where gradients are close to zero (e.g., saddle points or poor local minima).\n",
        "   - **Effect on Training**: The model may struggle to escape these flat regions, resulting in slow or stalled progress during training.\n",
        "   - **Consequence**: The model might end up converging to suboptimal solutions, reducing its performance on the task.\n",
        "\n",
        "### 5. **Symmetry Breaking Issues**\n",
        "   - **Problem**: Initializing all weights to the same value (such as zero) results in neurons in the same layer learning the same features. This happens because they receive identical gradients and updates during backpropagation.\n",
        "   - **Effect on Training**: The model is unable to learn diverse features, limiting its representational capacity and overall performance.\n",
        "   - **Consequence**: The neural network becomes less effective, often underfitting the data.\n",
        "\n",
        "### 6. **Bias in Gradient Flow**\n",
        "   - **Problem**: Improper initialization can cause a **bias** in how gradients flow across the network. Some neurons may have stronger gradients (due to larger initial weights), while others have weaker gradients.\n",
        "   - **Effect on Training**: This imbalance means that some neurons dominate learning while others contribute little, causing an unbalanced training process.\n",
        "   - **Consequence**: The network may focus too much on certain features or parts of the data, reducing its ability to generalize.\n",
        "\n",
        "### Conclusion:\n",
        "Improper weight initialization introduces a variety of challenges, from vanishing and exploding gradients to slow convergence and suboptimal learning. These issues hamper the model’s ability to learn effectively and generalize well to new data. Proper initialization techniques (like Xavier or He initialization) help mitigate these problems, leading to faster and more stable convergence."
      ],
      "metadata": {
        "id": "jpxNgiQp_-11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3) Discuss the concept of variance and how it relates to weight initialization. WhE is it crucial to consider the\n",
        "variance of weights during initialization?"
      ],
      "metadata": {
        "id": "q6xb0MHiABPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "In the context of weight initialization for artificial neural networks, **variance** refers to the spread or dispersion of weight values. Considering the variance of weights during initialization is crucial because it directly affects how the network learns, particularly through forward and backward propagation. Here’s a breakdown of why it’s essential and how it relates to weight initialization:\n",
        "\n",
        "### 1. **Variance and Stability of Activations**\n",
        "   - **Variance** determines how activations (outputs of neurons) propagate through the network. If weights are initialized with inappropriate variance, the magnitude of activations can either:\n",
        "     - **Shrink**: If the variance is too small, activations can tend toward zero as they move deeper into the network.\n",
        "     - **Grow**: If the variance is too large, activations can become excessively large.\n",
        "   - This can lead to problems like **vanishing** or **exploding activations**, causing the network to learn inefficiently or fail to learn entirely.\n",
        "\n",
        "### 2. **Forward Propagation: Controlling the Scale of Activations**\n",
        "   - During forward propagation, the activations from each layer are passed to the next layer. The goal is to keep these activations well-scaled so they neither vanish nor explode as they pass through layers, especially in deep networks.\n",
        "   - If the variance of weights is not controlled during initialization:\n",
        "     - **Too small variance**: The output of each layer might get smaller and smaller as it propagates, leading to vanishing activations.\n",
        "     - **Too large variance**: The output might grow too large, leading to exploding activations.\n",
        "   - **Why is this important?** Properly initializing the weights with appropriate variance helps to maintain stable activations across layers, which is key to effective training.\n",
        "\n",
        "### 3. **Backward Propagation: Controlling the Scale of Gradients**\n",
        "   - Backward propagation relies on gradients to update weights. The gradients are affected by the activations during the forward pass.\n",
        "   - If activations are too small or too large (due to poor weight initialization), the gradients will either:\n",
        "     - **Vanishing Gradients**: When weights have small variance, gradients can shrink exponentially as they propagate backward, causing the model to stop learning in deeper layers.\n",
        "     - **Exploding Gradients**: When weights have large variance, gradients can grow exponentially, causing erratic updates and instability.\n",
        "   - **Why is this important?** Correctly initializing weights with proper variance ensures stable gradients during backpropagation, preventing the vanishing or exploding gradient problem.\n",
        "\n",
        "### 4. **Relationship to Activation Functions**\n",
        "   - The appropriate variance of weights depends on the activation function being used. Different activation functions require different scales for the weights to ensure that activations and gradients remain well-scaled.\n",
        "   - For example:\n",
        "     - **Sigmoid or Tanh activation functions**: These functions squash outputs into a small range, so using small weight variances can exacerbate the vanishing gradient problem. **Xavier initialization** is commonly used in such cases, where weights are scaled to maintain a stable variance of activations across layers.\n",
        "     - **ReLU (Rectified Linear Unit)**: ReLU does not suffer from the same squashing issue, but large initial weights can still cause exploding gradients. **He initialization** (also known as Kaiming initialization) is used, where weights are scaled to keep the variance of activations constant across layers.\n",
        "\n",
        "### 5. **Ensuring Balanced Learning**\n",
        "   - Maintaining the correct variance in weights during initialization ensures that all layers learn in a balanced manner. If variance is too high or too low, some neurons might dominate the learning process, while others barely contribute.\n",
        "   - By controlling the variance, the network ensures that every neuron plays a role in the learning process, leading to better generalization.\n",
        "\n",
        "### 6. **Mathematical Insight: Variance in Common Initialization Methods**\n",
        "   - **Xavier Initialization (Glorot)**: Used for sigmoid or tanh activation functions, Xavier initialization sets the variance of the weights to ensure that the variance of activations is the same across all layers. The weights are typically drawn from a distribution with variance:\n",
        "     \\[\n",
        "     Var(W) = \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\n",
        "     \\]\n",
        "     where \\( n_{\\text{in}} \\) and \\( n_{\\text{out}} \\) are the number of input and output units of a layer, respectively.\n",
        "\n",
        "   - **He Initialization**: Used for ReLU or variants, He initialization sets the variance of the weights to:\n",
        "     \\[\n",
        "     Var(W) = \\frac{2}{n_{\\text{in}}}\n",
        "     \\]\n",
        "     This helps maintain the variance of activations across layers when using ReLU activations, which tend to \"drop\" half of the inputs (those that are negative).\n",
        "\n",
        "### Conclusion:\n",
        "It is crucial to consider the variance of weights during initialization because it determines the stability of activations and gradients during training. Proper variance ensures that the network avoids vanishing or exploding gradients, leading to more efficient and stable learning. Initialization methods like Xavier and He take into account the variance required for different activation functions, ensuring that the network can learn effectively across all layers."
      ],
      "metadata": {
        "id": "pNo5Qs-tAHWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Part 2: Weight Ipitializatiop Techpique"
      ],
      "metadata": {
        "id": "HdhdbFwhAsZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4) Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate\n",
        "to use"
      ],
      "metadata": {
        "id": "2dATkZYGAu3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Zero Initialization in Neural Networks\n",
        "\n",
        "**Zero initialization** refers to the practice of initializing all the weights of a neural network to zero. While this may seem like a simple and straightforward approach, it poses significant challenges for training and the learning capacity of a network, especially in deep learning contexts.\n",
        "\n",
        "### Key Concept:\n",
        "- **Zero initialization** sets all the weights in the neural network (between neurons or layers) to zero at the start of training.\n",
        "- During training, weights are updated based on the gradients computed from the loss function, with the goal of minimizing the error.\n",
        "\n",
        "However, zero initialization leads to several limitations, which make it inappropriate for most neural network training tasks.\n",
        "\n",
        "### Potential Limitations of Zero Initialization\n",
        "\n",
        "#### 1. **Symmetry Problem**\n",
        "   - **Main issue**: All neurons in a given layer will compute the same output and receive the same gradient during backpropagation if the weights are initialized to zero.\n",
        "   - **Why?**: Because all weights are zero, the inputs to each neuron in the same layer will be identical. Consequently, the gradients for each weight will also be identical, leading to the same weight updates for each neuron.\n",
        "   - **Effect on Learning**: This effectively makes all neurons in a given layer behave identically, removing any capacity for them to learn different features. It defeats the purpose of having multiple neurons in a layer, severely limiting the representational power of the network.\n",
        "   - **Consequence**: The network will fail to break symmetry, making it impossible to learn complex patterns and features. The model will not improve beyond the first iteration, essentially \"freezing\" the neurons with identical weights.\n",
        "\n",
        "#### 2. **Slow or No Convergence**\n",
        "   - Zero initialization prevents the network from updating weights in a meaningful way. Since the symmetry problem ensures that all neurons in a layer perform identical computations, the network cannot effectively learn and may not converge at all.\n",
        "   - **Effect on Training**: The model will either not learn or converge to a poor solution, as it cannot differentiate between different inputs and compute meaningful representations.\n",
        "\n",
        "#### 3. **No Diversity in Learning**\n",
        "   - Neurons in a network are meant to learn diverse features from the input data. With zero initialization, all neurons in a layer essentially perform the same operation, rendering the model incapable of learning multiple, distinct features.\n",
        "   - **Effect on Representation**: This severely limits the model’s ability to generalize to new data, as it lacks the capacity to capture diverse patterns in the input.\n",
        "\n",
        "### When Zero Initialization Might Be Appropriate\n",
        "\n",
        "Zero initialization is rarely appropriate for weight initialization in neural networks, especially when training deep models. However, there are a few cases where zero initialization or initialization of **biases** to zero might be acceptable:\n",
        "\n",
        "#### 1. **Bias Initialization**\n",
        "   - While initializing **weights** to zero is problematic, initializing **biases** to zero is often acceptable, especially in conjunction with proper weight initialization.\n",
        "   - Biases are added to the weighted sum of inputs and act as a shift in the activation function. Since their role is different from weights (which are multiplied with inputs), initializing biases to zero does not create the same symmetry issues.\n",
        "   - **Common Use**: Many initialization schemes (such as Xavier or He initialization) initialize weights randomly, but set biases to zero by default without affecting training.\n",
        "\n",
        "#### 2. **Linear Models (Without Hidden Layers)**\n",
        "   - In simple linear models (such as logistic regression or linear regression) that don’t have hidden layers, initializing weights to zero may not create the same issues as in deep neural networks.\n",
        "   - **Why?**: In these models, there are no multiple layers or neurons learning different features. However, even in this case, it's common to initialize weights randomly to avoid potential issues during optimization.\n",
        "\n",
        "#### 3. **Small, Highly Structured Problems**\n",
        "   - In some small-scale problems, particularly where the data or problem is highly structured (e.g., low-dimensional datasets or rule-based systems), zero initialization might not significantly hinder learning. However, this is rarely the case in real-world machine learning tasks.\n",
        "\n",
        "### Conclusion:\n",
        "Zero initialization is generally not recommended for the **weights** in neural networks due to the **symmetry problem**, which leads to identical neuron behavior and severely limits learning. However, zero initialization can be appropriate for **biases**, and in simpler linear models, the risks are lower. For deep learning, random weight initialization methods like Xavier or He initialization are far more effective at ensuring diverse feature learning and preventing issues such as vanishing gradients."
      ],
      "metadata": {
        "id": "zzxYvYZkBJVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5) Describe the process of random initialization. How can random initialization be adjusted to mitigate\n",
        "potential issues like saturation or vanishing/exploding gradients?"
      ],
      "metadata": {
        "id": "excrYAw6BLgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Random Initialization in Neural Networks\n",
        "\n",
        "**Random initialization** refers to the practice of assigning random values to the weights of a neural network at the start of training. This breaks the **symmetry** between neurons, allowing them to learn different features from the input data. Random initialization is one of the most commonly used techniques for weight initialization in deep learning models, especially in conjunction with specific strategies designed to prevent issues like **saturation**, **vanishing gradients**, or **exploding gradients**.\n",
        "\n",
        "### Process of Random Initialization\n",
        "\n",
        "1. **Assign Random Values to Weights**:\n",
        "   - Each weight in the network is assigned a random value, typically drawn from a uniform or normal (Gaussian) distribution. This ensures that different neurons within a layer start with different weights, which is critical for allowing the network to learn different features.\n",
        "\n",
        "2. **Bias Initialization**:\n",
        "   - While the weights are initialized randomly, biases are often initialized to zero or small random values. Since biases are added after the weighted sum, initializing them to zero typically doesn't lead to symmetry problems.\n",
        "\n",
        "### Key Benefits of Random Initialization\n",
        "- **Breaking Symmetry**: Neurons in the same layer will compute different outputs due to their different initial weights, preventing them from learning identical features.\n",
        "- **Avoiding Degeneracy**: Random initialization helps the network avoid degenerate solutions where neurons perform redundant computations.\n",
        "- **Improved Learning Efficiency**: By starting from different initial points, random initialization helps the optimization algorithm (e.g., gradient descent) explore more regions of the loss surface.\n",
        "\n",
        "### Potential Issues with Basic Random Initialization\n",
        "Despite its advantages, basic random initialization (where weights are sampled from a simple uniform or normal distribution) can lead to problems such as **saturation**, **vanishing gradients**, or **exploding gradients** during training.\n",
        "\n",
        "#### 1. **Saturation**:\n",
        "   - Saturation occurs when neurons in a layer output values that are in the flat regions of their activation function. For example, with sigmoid or tanh activations, large positive or negative inputs can push the outputs close to 1 or -1, where the gradient becomes very small.\n",
        "   - **Effect**: This causes neurons to stop learning because the gradients are near zero (known as the **vanishing gradient problem**).\n",
        "\n",
        "#### 2. **Vanishing and Exploding Gradients**:\n",
        "   - **Vanishing gradients**: If weights are initialized with very small values, the gradients (used in backpropagation) shrink as they propagate back through the layers, particularly in deep networks. This prevents the network from learning effectively in deeper layers.\n",
        "   - **Exploding gradients**: If weights are initialized with very large values, the gradients can grow exponentially during backpropagation, leading to unstable updates and causing the model to diverge.\n",
        "\n",
        "### Adjustments to Random Initialization: Techniques to Mitigate Issues\n",
        "\n",
        "To address the problems of saturation, vanishing gradients, and exploding gradients, researchers have developed several advanced random initialization techniques that adjust how the weights are distributed based on the network's architecture and activation functions.\n",
        "\n",
        "#### 1. **Xavier (Glorot) Initialization**\n",
        "   - **Purpose**: Designed to maintain a stable variance of activations and gradients throughout the layers, especially when using **sigmoid** or **tanh** activation functions.\n",
        "   - **How it works**: Xavier initialization adjusts the random weights to have a variance that depends on the number of input and output units for a given layer. The weights are drawn from a normal or uniform distribution with variance:\n",
        "     \\[\n",
        "     Var(W) = \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\n",
        "     \\]\n",
        "     where \\(n_{\\text{in}}\\) is the number of inputs to the layer, and \\(n_{\\text{out}}\\) is the number of outputs.\n",
        "   - **Effect**: By scaling the weights based on the size of the layer, Xavier initialization helps prevent both vanishing and exploding gradients. It ensures that the gradients remain in a reasonable range as they propagate through the network.\n",
        "   - **When to use**: Best suited for networks using sigmoid or tanh activation functions, which are prone to vanishing gradient issues.\n",
        "\n",
        "#### 2. **He (Kaiming) Initialization**\n",
        "   - **Purpose**: Developed to address issues when using **ReLU** activation functions, which tend to have neurons \"die\" (i.e., output zero for all inputs) if weights are too small, and can cause exploding gradients if weights are too large.\n",
        "   - **How it works**: He initialization scales the variance of the weights to depend only on the number of inputs to the layer. The weights are drawn from a distribution with variance:\n",
        "     \\[\n",
        "     Var(W) = \\frac{2}{n_{\\text{in}}}\n",
        "     \\]\n",
        "   - **Effect**: He initialization helps ensure that the activations remain well-scaled when using ReLU, preventing the gradients from vanishing or exploding. It also mitigates the problem of neurons dying early in training due to improper weight scaling.\n",
        "   - **When to use**: Best suited for networks using ReLU or its variants (e.g., Leaky ReLU, ELU) as the activation function.\n",
        "\n",
        "#### 3. **LeCun Initialization**\n",
        "   - **Purpose**: Similar to Xavier and He initialization but specifically designed for networks using **sigmoid** or **tanh** activations.\n",
        "   - **How it works**: LeCun initialization is based on the principle that the weights should be scaled such that the variance of the weights is inversely proportional to the number of inputs to the layer:\n",
        "     \\[\n",
        "     Var(W) = \\frac{1}{n_{\\text{in}}}\n",
        "     \\]\n",
        "   - **Effect**: LeCun initialization helps prevent saturation in sigmoid and tanh networks by keeping the initial weights in a range that reduces the chances of neurons being in the flat regions of the activation functions.\n",
        "   - **When to use**: Best for shallow networks or those using tanh/sigmoid activation functions.\n",
        "\n",
        "#### 4. **Layer-Specific Initialization**\n",
        "   - **Adaptive Initialization**: In some cases, it's helpful to adjust the initialization based on the layer type. For example, convolutional layers and fully connected layers may require different initialization strategies due to differences in their number of connections and neuron interactions.\n",
        "   - **Effect**: Layer-specific initialization can be more fine-tuned to account for the architecture and depth of the network, improving overall stability and training efficiency.\n",
        "\n",
        "### Conclusion:\n",
        "Random initialization is a critical step in training neural networks, but improper initialization can lead to issues like saturation, vanishing gradients, or exploding gradients. Adjustments such as **Xavier**, **He**, and **LeCun initialization** have been developed to ensure that weights are scaled appropriately, depending on the activation functions and architecture of the network. These techniques help maintain stable activations and gradients during forward and backward propagation, ensuring efficient and effective learning, especially in deep networks."
      ],
      "metadata": {
        "id": "QWnKEPEOBWvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6) Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper\n",
        "weight initialization and the underlEing theorE behind it"
      ],
      "metadata": {
        "id": "Fv55bXlTCT2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Xavier/Glorot Initialization\n",
        "\n",
        "**Xavier initialization**, also known as **Glorot initialization**, is a widely used method for initializing the weights of a neural network. It was introduced by Xavier Glorot and Yoshua Bengio in their 2010 paper, with the aim of addressing key challenges related to improper weight initialization, such as vanishing and exploding gradients. Xavier initialization provides a more stable way to initialize weights so that activations and gradients stay within a reasonable range as they propagate through the network.\n",
        "\n",
        "### Purpose of Xavier Initialization\n",
        "Xavier initialization is specifically designed to maintain the **scale of activations** and **gradients** consistent across layers, especially in deep networks. Without proper initialization, weights may cause the activations to either shrink or grow exponentially as they move through the network, leading to **vanishing** or **exploding gradients**, both of which severely hinder the training process.\n",
        "\n",
        "### Formula for Xavier Initialization\n",
        "Xavier initialization assumes that the weights are drawn from a distribution with a specific variance, which depends on the size of the layer, particularly the number of input and output units.\n",
        "\n",
        "The weights are initialized by drawing random values from either a **uniform** or **normal** distribution, where the variance is set to balance the inputs and outputs of each neuron.\n",
        "\n",
        "1. **Uniform Distribution**:\n",
        "   \\[\n",
        "   W \\sim U\\left(-\\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}\\right)\n",
        "   \\]\n",
        "   - Here, \\( W \\) represents the weight matrix.\n",
        "   - \\( n_{\\text{in}} \\) is the number of input units to the layer, and \\( n_{\\text{out}} \\) is the number of output units.\n",
        "   - The constant \\( \\sqrt{6} \\) is chosen to ensure a proper scaling of the variance for a uniform distribution.\n",
        "\n",
        "2. **Normal Distribution**:\n",
        "   \\[\n",
        "   W \\sim N\\left(0, \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}\\right)\n",
        "   \\]\n",
        "   - Here, \\( W \\) is drawn from a normal distribution with a mean of 0 and variance of \\( \\frac{2}{n_{\\text{in}} + n_{\\text{out}}} \\).\n",
        "\n",
        "The key idea is to scale the weights in a way that neither the activations nor the gradients explode or vanish as they pass through layers.\n",
        "\n",
        "### Addressing Challenges of Improper Weight Initialization\n",
        "\n",
        "#### 1. **Vanishing and Exploding Gradients**\n",
        "   - **Vanishing gradients** occur when the gradients become extremely small, especially in deep networks, causing the network to stop learning effectively. This is common when using **sigmoid** or **tanh** activation functions, as these functions squash the outputs into a limited range, leading to gradients that approach zero during backpropagation.\n",
        "   - **Exploding gradients** happen when the gradients grow uncontrollably as they propagate through layers, causing the weight updates to be excessively large, leading to unstable learning.\n",
        "\n",
        "   **How Xavier Initialization Solves This**:\n",
        "   - Xavier initialization carefully balances the variance of the weights so that the **input and output variance are equal** across layers. This keeps the magnitude of activations stable, preventing them from becoming too small (which would lead to vanishing gradients) or too large (which would lead to exploding gradients).\n",
        "   - By ensuring that the variance of the activations remains consistent, Xavier initialization mitigates the risk of gradients shrinking or growing as they propagate through the network.\n",
        "\n",
        "#### 2. **Saturation of Activation Functions**\n",
        "   - Saturation occurs when the inputs to activation functions like sigmoid or tanh are too large, pushing the activations into the flat regions of the function where the gradients are near zero. This leads to **slow learning** or **no learning** because the network is unable to update weights effectively.\n",
        "\n",
        "   **How Xavier Initialization Solves This**:\n",
        "   - Xavier initialization prevents the activations from entering the saturated regions of these functions by ensuring that the weights are small enough that the inputs remain in a range where the activation functions can output meaningful gradients.\n",
        "   - This allows the network to learn efficiently by keeping activations and gradients in a region where they have a non-zero slope, which helps avoid the vanishing gradient problem.\n",
        "\n",
        "### Underlying Theory Behind Xavier Initialization\n",
        "\n",
        "The underlying theory of Xavier initialization is based on the principle of **maintaining the variance** of both activations and gradients as they propagate through the network. This ensures stable learning by preventing gradients from vanishing or exploding, which is crucial in deep networks.\n",
        "\n",
        "#### 1. **Variance Preservation**\n",
        "   - The idea is to maintain the **variance of activations** across layers so that information flows smoothly from the input to the output, without amplifying or diminishing the signal as it passes through multiple layers.\n",
        "   - Consider a neuron that computes a weighted sum of its inputs:\n",
        "     \\[\n",
        "     z = W^T x + b\n",
        "     \\]\n",
        "     where \\( W \\) is the weight matrix, \\( x \\) is the input, and \\( b \\) is the bias.\n",
        "   - For effective learning, the variance of the output \\( z \\) should be similar to the variance of the input \\( x \\), ensuring that the signal doesn't shrink or grow too much.\n",
        "\n",
        "   - Xavier initialization derives the variance of weights that ensures this balance, based on the assumption that both input and output neurons should contribute equally to the variance of activations.\n",
        "\n",
        "#### 2. **Forward and Backward Propagation**\n",
        "   - **Forward Propagation**: The goal is to keep the variance of the outputs (activations) the same across layers. If the variance of the activations grows or shrinks too much, it can lead to saturation or vanishing/exploding activations.\n",
        "   - **Backward Propagation**: The gradients during backpropagation should also maintain a reasonable variance. If the weights are too large or too small, the gradients can either explode (leading to large updates and instability) or vanish (causing the network to stop learning).\n",
        "\n",
        "### Summary: How Xavier Initialization Works\n",
        "1. **Equalizing Variance**: Xavier initialization balances the variance of weights so that the variance of the input and output activations are similar, preventing the activations from becoming too large or small.\n",
        "2. **Stable Gradient Flow**: By keeping the variance stable, the gradients during backpropagation are also kept within a manageable range, avoiding the problems of vanishing or exploding gradients.\n",
        "3. **Suitable for Sigmoid and Tanh**: Xavier initialization is particularly effective for networks using sigmoid or tanh activations, which are more prone to vanishing gradients due to their saturation regions.\n",
        "\n",
        "### Conclusion:\n",
        "Xavier/Glorot initialization addresses the critical challenges of improper weight initialization—such as vanishing and exploding gradients—by ensuring that weights are initialized with a specific variance based on the size of the network layers. This leads to more stable forward and backward propagation, allowing deep networks to train more efficiently. The theory behind Xavier initialization revolves around preserving the variance of activations and gradients across layers, which is essential for deep learning models to converge and perform well."
      ],
      "metadata": {
        "id": "uNLqpMHNCZbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7) Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it\n",
        "preferred?"
      ],
      "metadata": {
        "id": "CN9vU1WWCqBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### He Initialization: Overview and Concept\n",
        "\n",
        "**He initialization**, also known as **Kaiming initialization**, was introduced by Kaiming He et al. in 2015. It is an advanced weight initialization technique specifically designed for neural networks that use **ReLU** (Rectified Linear Unit) or its variants (e.g., Leaky ReLU) as activation functions. The primary goal of He initialization is to address issues such as **vanishing gradients** and **dead neurons**, which are more common when using ReLU activations.\n",
        "\n",
        "### Formula for He Initialization\n",
        "He initialization uses a formula similar to Xavier initialization but with a crucial difference in scaling to accommodate the properties of ReLU activations. The weights are initialized with a variance that depends only on the number of input units in each layer.\n",
        "\n",
        "1. **Uniform Distribution**:\n",
        "   \\[\n",
        "   W \\sim U\\left(-\\sqrt{\\frac{6}{n_{\\text{in}}}}, \\sqrt{\\frac{6}{n_{\\text{in}}}}\\right)\n",
        "   \\]\n",
        "   - The range for uniform distribution is scaled based on \\( n_{\\text{in}} \\), the number of input units to the layer.\n",
        "\n",
        "2. **Normal Distribution**:\n",
        "   \\[\n",
        "   W \\sim N\\left(0, \\frac{2}{n_{\\text{in}}}\\right)\n",
        "   \\]\n",
        "   - In this case, \\( W \\) is drawn from a normal distribution with a mean of 0 and variance \\( \\frac{2}{n_{\\text{in}}} \\), where \\( n_{\\text{in}} \\) is the number of inputs to the layer.\n",
        "\n",
        "The key difference between He initialization and Xavier initialization is that **He initialization scales the weights by a factor of 2**, which is particularly important for **ReLU**-based networks, as ReLU activations \"kill\" half of the inputs (by turning them into zeros when the input is negative).\n",
        "\n",
        "### Difference Between He Initialization and Xavier Initialization\n",
        "\n",
        "#### 1. **Scaling Factor**\n",
        "   - **Xavier initialization** scales the variance of the weights by \\( \\frac{1}{n_{\\text{in}} + n_{\\text{out}}} \\), ensuring that the variance of the activations remains balanced between input and output units. It is designed for **sigmoid** and **tanh** activation functions, which are symmetric and can output both positive and negative values.\n",
        "   - **He initialization** scales the variance by \\( \\frac{2}{n_{\\text{in}}} \\), which is higher than in Xavier initialization. This compensates for the fact that **ReLU** activation functions only output positive values, causing half of the activations to be zero. The increased variance ensures that the non-zero activations remain sufficiently large to prevent vanishing gradients.\n",
        "\n",
        "#### 2. **Target Activation Functions**\n",
        "   - **Xavier initialization** is ideal for **sigmoid** and **tanh** activations, which are bounded and can saturate (flatten) at their extremes, making it critical to balance the variance between layers.\n",
        "   - **He initialization** is specifically designed for **ReLU** and **ReLU variants** (such as Leaky ReLU and Parametric ReLU), which output positive values and can cause \"dead neurons\" (neurons that output zero for all inputs). The higher variance in He initialization addresses this by ensuring that the activations are scaled properly for ReLU networks.\n",
        "\n",
        "#### 3. **Handling of Activation Saturation**\n",
        "   - **Xavier initialization** aims to keep the activations from saturating in sigmoid and tanh networks, where inputs can get stuck in the flat regions of the activation functions (with near-zero gradients).\n",
        "   - **He initialization** focuses on preventing neurons from dying (where neurons output zeros indefinitely) by ensuring that activations in ReLU networks are sufficiently large and positive.\n",
        "\n",
        "### Why He Initialization is Preferred for ReLU Networks\n",
        "\n",
        "ReLU activations introduce specific challenges during training due to their nature:\n",
        "- **ReLU \"kills\" half of the activations** by turning negative values into zeros. While this sparsity can help with computational efficiency, it also means that neurons can become \"dead\" (i.e., always output zero) if the weights are not initialized properly.\n",
        "- **Dead neurons**: If neurons output zero consistently, they stop contributing to the learning process, resulting in poor performance and reduced network capacity.\n",
        "- **Vanishing gradients**: Without proper initialization, the network may suffer from vanishing gradients, where weight updates become very small, making it difficult for deeper layers to learn.\n",
        "\n",
        "**He initialization** addresses these issues by setting the weights with a variance of \\( \\frac{2}{n_{\\text{in}}} \\), compensating for the ReLU activation’s tendency to zero-out negative values. This helps ensure that neurons are more likely to output non-zero values and continue learning.\n",
        "\n",
        "### When He Initialization is Preferred\n",
        "\n",
        "1. **ReLU Activation Functions**:\n",
        "   He initialization is explicitly designed for neural networks using **ReLU** and its variants (Leaky ReLU, Parametric ReLU, etc.). ReLU activation functions are widely used in deep learning because of their computational efficiency and ability to reduce the likelihood of vanishing gradients. He initialization optimizes weight scaling for these activations, allowing for faster convergence and better training results.\n",
        "\n",
        "2. **Deep Networks**:\n",
        "   In very **deep networks**, where vanishing gradients can be particularly problematic, He initialization helps propagate stronger signals through the network. It is especially effective for networks with many layers, as the scaling factor ensures that activations remain sufficiently large even in deeper layers.\n",
        "\n",
        "3. **Computer Vision Tasks**:\n",
        "   He initialization is often used in convolutional neural networks (CNNs), which frequently use ReLU activations. CNNs are prevalent in computer vision tasks (e.g., image classification, object detection) due to their ability to extract hierarchical feature representations. He initialization helps maintain the effectiveness of these layers by preventing dead neurons and ensuring stable gradients.\n",
        "\n",
        "### Summary of He vs. Xavier Initialization\n",
        "\n",
        "| **Aspect**                 | **Xavier Initialization**                              | **He Initialization**                               |\n",
        "|----------------------------|--------------------------------------------------------|-----------------------------------------------------|\n",
        "| **Target Activation**       | Sigmoid, Tanh                                          | ReLU, Leaky ReLU, Parametric ReLU                    |\n",
        "| **Weight Variance**         | \\( \\frac{1}{n_{\\text{in}} + n_{\\text{out}}} \\)         | \\( \\frac{2}{n_{\\text{in}}} \\)                       |\n",
        "| **Purpose**                 | Prevent vanishing/exploding gradients for saturating functions (sigmoid/tanh) | Prevent dead neurons and maintain activations for ReLU |\n",
        "| **When to Use**             | Shallow networks, sigmoid/tanh activations             | Deep networks, ReLU/Leaky ReLU activations           |\n",
        "| **Primary Problem Addressed** | Vanishing gradients (sigmoid/tanh saturation)          | Dead neurons and vanishing gradients (ReLU)          |\n",
        "\n",
        "### Conclusion:\n",
        "**He initialization** is preferred for networks using **ReLU** and its variants because it helps prevent neurons from becoming \"dead\" and ensures that gradients remain in a healthy range for backpropagation. It differs from **Xavier initialization** in that it scales weights with a higher variance to accommodate the unique properties of ReLU activations. For deep networks, especially in applications like computer vision, He initialization ensures better training stability and faster convergence."
      ],
      "metadata": {
        "id": "cs01ExT8CvJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Part 3: Applyipg Weight Ipitializatioo"
      ],
      "metadata": {
        "id": "9LstLw3bDITe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8) Implement different weight initialization techniques (zero initialization, random initialization, Xavier\n",
        "initialization, and He initialization) in a neural network using a framework of Eour choice. Train the model\n",
        "on a suitable dataset and compare the performance of the initialized modelsk"
      ],
      "metadata": {
        "id": "-jGhrI_kDhHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "pzDT9PEjDkz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nnA7U4vDpM2",
        "outputId": "1c119d57-9d0b-4de1-865d-df3c6640eb3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple neural network model\n",
        "def create_model(weight_initializer):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=(28, 28, 1)))\n",
        "    model.add(layers.Dense(128, kernel_initializer=weight_initializer, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "HEsdDanhDyHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate models with different weight initializations\n",
        "initializers = ['zeros', 'random_normal', 'glorot_normal', 'he_normal']\n",
        "histories = []"
      ],
      "metadata": {
        "id": "hhnVmIR_D0V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for initializer in initializers:\n",
        "    print(f\"\\nTraining model with {initializer} initialization:\")\n",
        "    model = create_model(initializer)\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2, verbose=2)\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "    print(f\"Test accuracy with {initializer} initialization: {test_acc}\")\n",
        "\n",
        "    histories.append((initializer, history))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCRJB3DTD2zR",
        "outputId": "15664abd-7e7c-4d6a-cb9c-444022f08a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with zeros initialization:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "750/750 - 4s - 5ms/step - accuracy: 0.1135 - loss: 2.3015 - val_accuracy: 0.1060 - val_loss: 2.3020\n",
            "Epoch 2/10\n",
            "750/750 - 3s - 3ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3022\n",
            "Epoch 3/10\n",
            "750/750 - 3s - 5ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3022\n",
            "Epoch 4/10\n",
            "750/750 - 4s - 5ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3021\n",
            "Epoch 5/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3021\n",
            "Epoch 6/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3023\n",
            "Epoch 7/10\n",
            "750/750 - 5s - 6ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3020\n",
            "Epoch 8/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3022\n",
            "Epoch 9/10\n",
            "750/750 - 3s - 3ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3022\n",
            "Epoch 10/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.1140 - loss: 2.3011 - val_accuracy: 0.1060 - val_loss: 2.3021\n",
            "313/313 - 0s - 1ms/step - accuracy: 0.1135 - loss: 2.3010\n",
            "Test accuracy with zeros initialization: 0.11349999904632568\n",
            "\n",
            "Training model with random_normal initialization:\n",
            "Epoch 1/10\n",
            "750/750 - 3s - 5ms/step - accuracy: 0.9073 - loss: 0.3332 - val_accuracy: 0.9507 - val_loss: 0.1744\n",
            "Epoch 2/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9566 - loss: 0.1497 - val_accuracy: 0.9621 - val_loss: 0.1302\n",
            "Epoch 3/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9695 - loss: 0.1066 - val_accuracy: 0.9657 - val_loss: 0.1127\n",
            "Epoch 4/10\n",
            "750/750 - 3s - 5ms/step - accuracy: 0.9771 - loss: 0.0805 - val_accuracy: 0.9715 - val_loss: 0.0980\n",
            "Epoch 5/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9813 - loss: 0.0643 - val_accuracy: 0.9719 - val_loss: 0.0927\n",
            "Epoch 6/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9852 - loss: 0.0510 - val_accuracy: 0.9723 - val_loss: 0.0909\n",
            "Epoch 7/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9881 - loss: 0.0412 - val_accuracy: 0.9730 - val_loss: 0.0861\n",
            "Epoch 8/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9906 - loss: 0.0331 - val_accuracy: 0.9738 - val_loss: 0.0910\n",
            "Epoch 9/10\n",
            "750/750 - 4s - 6ms/step - accuracy: 0.9920 - loss: 0.0274 - val_accuracy: 0.9739 - val_loss: 0.0873\n",
            "Epoch 10/10\n",
            "750/750 - 3s - 3ms/step - accuracy: 0.9942 - loss: 0.0222 - val_accuracy: 0.9742 - val_loss: 0.0871\n",
            "313/313 - 0s - 1ms/step - accuracy: 0.9768 - loss: 0.0765\n",
            "Test accuracy with random_normal initialization: 0.9768000245094299\n",
            "\n",
            "Training model with glorot_normal initialization:\n",
            "Epoch 1/10\n",
            "750/750 - 3s - 5ms/step - accuracy: 0.9086 - loss: 0.3296 - val_accuracy: 0.9476 - val_loss: 0.1814\n",
            "Epoch 2/10\n",
            "750/750 - 5s - 6ms/step - accuracy: 0.9565 - loss: 0.1513 - val_accuracy: 0.9621 - val_loss: 0.1342\n",
            "Epoch 3/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9685 - loss: 0.1075 - val_accuracy: 0.9670 - val_loss: 0.1092\n",
            "Epoch 4/10\n",
            "750/750 - 3s - 3ms/step - accuracy: 0.9762 - loss: 0.0820 - val_accuracy: 0.9694 - val_loss: 0.1021\n",
            "Epoch 5/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9805 - loss: 0.0650 - val_accuracy: 0.9724 - val_loss: 0.0923\n",
            "Epoch 6/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9850 - loss: 0.0528 - val_accuracy: 0.9710 - val_loss: 0.0957\n",
            "Epoch 7/10\n",
            "750/750 - 4s - 6ms/step - accuracy: 0.9880 - loss: 0.0416 - val_accuracy: 0.9727 - val_loss: 0.0887\n",
            "Epoch 8/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9899 - loss: 0.0347 - val_accuracy: 0.9757 - val_loss: 0.0827\n",
            "Epoch 9/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9920 - loss: 0.0283 - val_accuracy: 0.9741 - val_loss: 0.0865\n",
            "Epoch 10/10\n",
            "750/750 - 4s - 5ms/step - accuracy: 0.9944 - loss: 0.0216 - val_accuracy: 0.9743 - val_loss: 0.0914\n",
            "313/313 - 0s - 1ms/step - accuracy: 0.9753 - loss: 0.0828\n",
            "Test accuracy with glorot_normal initialization: 0.9753000140190125\n",
            "\n",
            "Training model with he_normal initialization:\n",
            "Epoch 1/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9064 - loss: 0.3328 - val_accuracy: 0.9493 - val_loss: 0.1808\n",
            "Epoch 2/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9552 - loss: 0.1525 - val_accuracy: 0.9582 - val_loss: 0.1389\n",
            "Epoch 3/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9688 - loss: 0.1056 - val_accuracy: 0.9678 - val_loss: 0.1107\n",
            "Epoch 4/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9765 - loss: 0.0810 - val_accuracy: 0.9690 - val_loss: 0.0999\n",
            "Epoch 5/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9809 - loss: 0.0637 - val_accuracy: 0.9722 - val_loss: 0.0916\n",
            "Epoch 6/10\n",
            "750/750 - 3s - 3ms/step - accuracy: 0.9851 - loss: 0.0513 - val_accuracy: 0.9722 - val_loss: 0.0933\n",
            "Epoch 7/10\n",
            "750/750 - 2s - 3ms/step - accuracy: 0.9887 - loss: 0.0408 - val_accuracy: 0.9724 - val_loss: 0.0905\n",
            "Epoch 8/10\n",
            "750/750 - 3s - 4ms/step - accuracy: 0.9898 - loss: 0.0348 - val_accuracy: 0.9726 - val_loss: 0.0886\n",
            "Epoch 9/10\n",
            "750/750 - 4s - 6ms/step - accuracy: 0.9921 - loss: 0.0278 - val_accuracy: 0.9743 - val_loss: 0.0875\n",
            "Epoch 10/10\n",
            "750/750 - 3s - 3ms/step - accuracy: 0.9937 - loss: 0.0230 - val_accuracy: 0.9760 - val_loss: 0.0849\n",
            "313/313 - 0s - 1ms/step - accuracy: 0.9768 - loss: 0.0771\n",
            "Test accuracy with he_normal initialization: 0.9768000245094299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the performance using plots or other metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for initializer, history in histories:\n",
        "    plt.plot(history.history['val_loss'], label=f'{initializer} initialization')\n",
        "\n",
        "plt.title('Validation Loss During Training')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Categorical Cross Entropy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "qxb138FQD5Ko",
        "outputId": "c519badb-d917-4a24-9586-e77417b4a713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Q9. Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique for a given neural network architecture and task.\n",
        "---"
      ],
      "metadata": {
        "id": "z21pFW4fEdot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Choosing the appropriate weight initialization technique for a neural network is crucial for successful training. Different initialization methods have varying impacts on convergence, performance, and the ability of a neural network to learn. Here are some considerations and tradeoffs to keep in mind:\n",
        "\n",
        "1. `Activation Function:`\n",
        "   - Consider the activation function used in the network. For example, He initialization works well with ReLU and its variants, while Xavier initialization is suitable for sigmoid and tanh activations. Using an appropriate initialization for the activation function helps in mitigating issues like vanishing or exploding gradients.\n",
        "\n",
        "2. `Network Architecture:`\n",
        "   - The depth and complexity of the neural network play a role in the choice of initialization. Deeper networks may benefit from initialization methods that help in addressing the vanishing/exploding gradient problem. He initialization is often preferred for deeper networks.\n",
        "\n",
        "3. `Task Requirements:`\n",
        "   - The nature of the task (e.g., classification, regression) may influence the choice of initialization. Some tasks may require more non-linearity, and activation functions like ReLU with He initialization can be beneficial.\n",
        "\n",
        "4. `Learning Rate and Optimization Algorithm:`\n",
        "   - The choice of weight initialization interacts with the learning rate and optimization algorithm. Some initialization methods may work better with specific learning rates or optimization algorithms. Experimentation and tuning are essential to find the optimal combination.\n",
        "\n",
        "5. `Batch Normalization:`\n",
        "   - If batch normalization is used in the network, the choice of weight initialization may interact with its effects. For example, He initialization is often used in conjunction with batch normalization.\n",
        "\n",
        "6. `Transfer Learning:`\n",
        "   - In transfer learning scenarios, where pre-trained models are used as a starting point, the choice of weight initialization in the pre-trained model might impact the fine-tuning process. It's essential to use the same or compatible initialization for consistency.\n",
        "\n",
        "7. `Computational Efficiency:`\n",
        "   - Some weight initialization methods may have additional computational costs compared to simpler methods. This consideration becomes important in resource-constrained environments.\n",
        "\n",
        "8. `Empirical Evaluation:`\n",
        "   - It's often beneficial to empirically evaluate different initialization methods on a specific task. Train models with different initializations and monitor their performance on training and validation sets. This can help identify which initialization method works best for a given scenario.\n",
        "\n",
        "9. `Stability During Training:`\n",
        "   - Consider the stability of the training process. Unstable training, as indicated by large fluctuations in loss or accuracy, may suggest that the chosen initialization is not suitable. Adjustments, such as changing the initialization or modifying the architecture, may be necessary.\n",
        "\n",
        "### In summary, there is no one-size-fits-all initialization method, and the choice depends on the specific characteristics of the network, the task at hand, and empirical observations. Experimentation and understanding the interactions between different components of the neural network are key to making informed decisions about weight initialization.\n",
        "\n",
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "scBGoCNUEhGT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}